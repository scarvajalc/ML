{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2  - Exercises 4  5\n",
    "## Modified by: Santiago Carvajal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPAM Detection - The Naive Bayes Algorithm in Python with Scikit-Learn \n",
    "D. Shahrokhian\n",
    "https://stackabuse.com/the-naive-bayes-algorithm-in-python-with-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 13.1\n",
      "  label                              message\n",
      "0   yes              Chinese Beijing Chinese\n",
      "1   yes             Chinese Chinese Shanghai\n",
      "2   yes                        Chinese Macao\n",
      "3    no                  Tokyo Japan Chinese\n",
      "4   yes  Chinese Chinese Chinese Tokyo Japan\n",
      "\n",
      "Table 13.10\n",
      "  label                message\n",
      "0   yes         Taipei Taiwan \n",
      "1   yes  Macao Taiwan Shanghai\n",
      "2    no           apan Sapporo\n",
      "3    no   Sapporo Osaka Taiwan\n",
      "4    no  Taiwan Taiwan Sapporo\n",
      "\n",
      "IAML Data\n",
      "  label                message\n",
      "0  spam  send us your password\n",
      "1   ham    send us your review\n",
      "2   ham   review your password\n",
      "3  spam              review us\n",
      "4  spam     send your password\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading data from Table 13.1\n",
    "df_131 = pd.read_table('Tabla131',  \n",
    "                   sep='\\t', \n",
    "                   header=None,\n",
    "                   names=['label', 'message'])\n",
    "# Reading data from Table 13.1\n",
    "df_1310 = pd.read_table('Tabla1310',  \n",
    "                   sep='\\t', \n",
    "                   header=None,\n",
    "                   names=['label', 'message'])\n",
    "# Reading data from IAML\n",
    "df_iaml = pd.read_table('IAML',  \n",
    "                   sep='\\t', \n",
    "                   header=None,\n",
    "                   names=['label', 'message'])\n",
    "print(\"Table 13.1\")\n",
    "print(df_131.head())\n",
    "print(\"\\nTable 13.10\")\n",
    "print(df_1310.head())\n",
    "print(\"\\nIAML Data\")\n",
    "print(df_iaml.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 13.1\n",
      "   label                              message\n",
      "0      1              chinese beijing chinese\n",
      "1      1             chinese chinese shanghai\n",
      "2      1                        chinese macao\n",
      "3      0                  tokyo japan chinese\n",
      "4      1  chinese chinese chinese tokyo japan\n",
      "\n",
      "Table 13.10\n",
      "   label                message\n",
      "0      1         taipei taiwan \n",
      "1      1  macao taiwan shanghai\n",
      "2      0           apan sapporo\n",
      "3      0   sapporo osaka taiwan\n",
      "4      0  taiwan taiwan sapporo\n",
      "\n",
      "IAML data\n",
      "   label                message\n",
      "0      1  send us your password\n",
      "1      0    send us your review\n",
      "2      0   review your password\n",
      "3      1              review us\n",
      "4      1     send your password\n"
     ]
    }
   ],
   "source": [
    "df_131['label'] = df_131.label.map({'no': 0, 'yes': 1})\n",
    "df_131['message'] = df_131.message.map(lambda x: x.lower())\n",
    "df_131['message'] = df_131.message.str.replace('[^\\w\\s]', '')\n",
    "print(\"Table 13.1\")\n",
    "print(df_131.head())\n",
    "\n",
    "df_1310['label'] = df_1310.label.map({'no': 0, 'yes': 1})\n",
    "df_1310['message'] = df_1310.message.map(lambda x: x.lower())\n",
    "df_1310['message'] = df_1310.message.str.replace('[^\\w\\s]', '')\n",
    "print(\"\\nTable 13.10\")\n",
    "print(df_1310.head())\n",
    "\n",
    "df_iaml['label'] = df_iaml.label.map({'ham': 0, 'spam': 1})\n",
    "df_iaml['message'] = df_iaml.message.map(lambda x: x.lower())\n",
    "df_iaml['message'] = df_iaml.message.str.replace('[^\\w\\s]', '')\n",
    "print(\"\\nIAML data\")\n",
    "print(df_iaml.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Santiago\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Table 13.1\n",
      "   label                                    message\n",
      "0      1                [chinese, beijing, chinese]\n",
      "1      1               [chinese, chinese, shanghai]\n",
      "2      1                           [chinese, macao]\n",
      "3      0                    [tokyo, japan, chinese]\n",
      "4      1  [chinese, chinese, chinese, tokyo, japan]\n",
      "\n",
      "Table 13.10\n",
      "   label                    message\n",
      "0      1           [taipei, taiwan]\n",
      "1      1  [macao, taiwan, shanghai]\n",
      "2      0            [apan, sapporo]\n",
      "3      0   [sapporo, osaka, taiwan]\n",
      "4      0  [taiwan, taiwan, sapporo]\n",
      "\n",
      "IAML data\n",
      "   label                     message\n",
      "0      1  [send, us, your, password]\n",
      "1      0    [send, us, your, review]\n",
      "2      0    [review, your, password]\n",
      "3      1                [review, us]\n",
      "4      1      [send, your, password]\n"
     ]
    }
   ],
   "source": [
    "# https://www.nltk.org/ Natural Language Toolkit\n",
    "# Punkt Sentence Tokenizer https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "df_131['message'] = df_131['message'].apply(nltk.word_tokenize)\n",
    "print(\"Table 13.1\")\n",
    "print(df_131.head())\n",
    "df_1310['message'] = df_1310['message'].apply(nltk.word_tokenize)\n",
    "print(\"\\nTable 13.10\")\n",
    "print(df_1310.head())\n",
    "df_iaml['message'] = df_iaml['message'].apply(nltk.word_tokenize)\n",
    "print(\"\\nIAML data\")\n",
    "print(df_iaml.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 13.1\n",
      "   label                                 message\n",
      "0      1                  [chines, beij, chines]\n",
      "1      1              [chines, chines, shanghai]\n",
      "2      1                         [chines, macao]\n",
      "3      0                  [tokyo, japan, chines]\n",
      "4      1  [chines, chines, chines, tokyo, japan]\n",
      "\n",
      "Table 13.10\n",
      "   label                    message\n",
      "0      1           [taipei, taiwan]\n",
      "1      1  [macao, taiwan, shanghai]\n",
      "2      0            [apan, sapporo]\n",
      "3      0   [sapporo, osaka, taiwan]\n",
      "4      0  [taiwan, taiwan, sapporo]\n",
      "\n",
      "IAML data\n",
      "   label                     message\n",
      "0      1  [send, us, your, password]\n",
      "1      0    [send, us, your, review]\n",
      "2      0    [review, your, password]\n",
      "3      1                [review, us]\n",
      "4      1      [send, your, password]\n"
     ]
    }
   ],
   "source": [
    "# https://www.nltk.org/api/nltk.stem.html\n",
    "#https://tartarus.org/martin/PorterStemmer/\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "df_131['message'] = df_131['message'].apply(lambda x: [stemmer.stem(y) for y in x]) \n",
    "print(\"Table 13.1\")\n",
    "print(df_131.head())\n",
    "df_1310['message'] = df_1310['message'].apply(lambda x: [stemmer.stem(y) for y in x]) \n",
    "print(\"\\nTable 13.10\")\n",
    "print(df_1310.head())\n",
    "df_iaml['message'] = df_iaml['message'].apply(lambda x: [stemmer.stem(y) for y in x]) \n",
    "print(\"\\nIAML data\")\n",
    "print(df_iaml.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 13.1\n",
      "   label                           message\n",
      "0      1                chines beij chines\n",
      "1      1            chines chines shanghai\n",
      "2      1                      chines macao\n",
      "3      0                tokyo japan chines\n",
      "4      1  chines chines chines tokyo japan\n",
      "\n",
      "Table 13.10\n",
      "   label                message\n",
      "0      1          taipei taiwan\n",
      "1      1  macao taiwan shanghai\n",
      "2      0           apan sapporo\n",
      "3      0   sapporo osaka taiwan\n",
      "4      0  taiwan taiwan sapporo\n",
      "\n",
      "IAML data\n",
      "   label                message\n",
      "0      1  send us your password\n",
      "1      0    send us your review\n",
      "2      0   review your password\n",
      "3      1              review us\n",
      "4      1     send your password\n"
     ]
    }
   ],
   "source": [
    "# Converts the list of words into space-separated strings\n",
    "df_131['message'] = df_131['message'].apply(lambda x: ' '.join(x))\n",
    "print(\"Table 13.1\")\n",
    "print(df_131.head())\n",
    "df_1310['message'] = df_1310['message'].apply(lambda x: ' '.join(x))\n",
    "print(\"\\nTable 13.10\")\n",
    "print(df_1310.head())\n",
    "df_iaml['message'] = df_iaml['message'].apply(lambda x: ' '.join(x))\n",
    "print(\"\\nIAML data\")\n",
    "print(df_iaml.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Table 13.1 data------------------------------------\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t2\n",
      "  (1, 4)\t1\n",
      "  (1, 1)\t2\n",
      "  (2, 3)\t1\n",
      "  (2, 1)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 2)\t1\n",
      "  (4, 5)\t1\n",
      "  (4, 1)\t3\n",
      "--------Table 13.10 data------------------------------------\n",
      "  (0, 6)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 6)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 0)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (4, 3)\t1\n",
      "  (4, 6)\t2\n",
      "--------IAML data------------------------------------\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 4)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 6)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 5)\t1\n",
      "  (4, 2)\t1\n",
      "  (4, 6)\t1\n",
      "  (4, 4)\t1\n",
      "  (5, 0)\t1\n",
      "  (5, 6)\t1\n",
      "  (5, 5)\t1\n",
      "  (5, 4)\t1\n",
      "  (6, 1)\t1\n",
      "  (6, 3)\t1\n",
      "  (6, 5)\t1\n"
     ]
    }
   ],
   "source": [
    "# Convert a collection of text documents to a matrix of token counts\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "# to allow one letter words count_vect = CountVectorizer(token_pattern = r\"(?u)\\b\\w+\\b\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(token_pattern = r\"(?u)\\b\\w+\\b\") \n",
    "counts_131 = count_vect.fit_transform(df_131['message'])  \n",
    "counts_1310 = count_vect.fit_transform(df_1310['message']) \n",
    "counts_iaml = count_vect.fit_transform(df_iaml['message']) \n",
    "\n",
    "print(\"--------Table 13.1 data------------------------------------\")\n",
    "print(counts_131)\n",
    "print(\"--------Table 13.10 data------------------------------------\")\n",
    "print(counts_1310)\n",
    "print(\"--------IAML data------------------------------------\")\n",
    "print(counts_iaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 6)\n",
      "(5, 7)\n",
      "(7, 7)\n"
     ]
    }
   ],
   "source": [
    "print(counts_131.shape)\n",
    "print(counts_1310.shape)\n",
    "print(counts_iaml.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutlinomial Naive Bayes Classification with sklearn ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Table 13.1 data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Classification with Table 13.1 data----\n",
      "Probabilities for each class \n",
      "[[0.31024139 0.68975861]]\n",
      "Accuracy: (0 - 1)\n",
      "1.0\n",
      "Confusion matrix: \n",
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/28064634/random-state-pseudo-random-numberin-scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "\n",
    "#Classiification with Table 13.1 data\n",
    "print(\"----Classification with Table 13.1 data----\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts_131, df_131['label'], test_size=0.2, shuffle = False ,random_state=69) \n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)  \n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "predicted_proba = model.predict_proba(X_test)\n",
    "print(\"Probabilities for each class \")\n",
    "print(predicted_proba)\n",
    "print(\"Accuracy: (0 - 1)\")\n",
    "print(np.mean(predicted == y_test))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion matrix: \")\n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Table 13.10 data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Classification with Table 13.10 data----\n",
      "Probabilities for each class \n",
      "[[0.57142857 0.42857143]]\n",
      "Accuracy: (0 - 1)\n",
      "1.0\n",
      "Confusion matrix: \n",
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/28064634/random-state-pseudo-random-numberin-scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "\n",
    "#Classiification with Table 13.1 data\n",
    "print(\"----Classification with Table 13.10 data----\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts_1310, df_1310['label'], test_size=0.2, shuffle = False ,random_state=69) \n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)  \n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "predicted_proba = model.predict_proba(X_test)\n",
    "print(\"Probabilities for each class \")\n",
    "print(predicted_proba)\n",
    "print(\"Accuracy: (0 - 1)\")\n",
    "print(np.mean(predicted == y_test))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion matrix: \")\n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with IAML data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Classification with IAML data----\n",
      "Probabilities for each class \n",
      "[[0.52228412 0.47771588]]\n",
      "Accuracy: (0 - 1)\n",
      "1.0\n",
      "Confusion matrix: \n",
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/28064634/random-state-pseudo-random-numberin-scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "\n",
    "#Classiification with Table 13.1 data\n",
    "print(\"----Classification with IAML data----\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts_iaml, df_iaml['label'], test_size=0.1, shuffle = False ,random_state=69) \n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)  \n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "predicted_proba = model.predict_proba(X_test)\n",
    "print(\"Probabilities for each class \")\n",
    "print(predicted_proba)\n",
    "print(\"Accuracy: (0 - 1)\")\n",
    "print(np.mean(predicted == y_test))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion matrix: \")\n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis ###\n",
    "We get different probabilities using the sklearn Naive Bayes library because they perform a normalization proccess before returning the final results of the probability of each class. Thats why we see bigger probabilities values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 5: Bernoulli Naive Bayes Classification with sklearn ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Table 13.1 data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Classification with Table 13.1 data----\n",
      "Probabilities for each class \n",
      "[[0.80893321 0.19106679]]\n",
      "Accuracy: (0 - 1)\n",
      "0.0\n",
      "Confusion matrix: \n",
      "[[0 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/28064634/random-state-pseudo-random-numberin-scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import numpy as np\n",
    "\n",
    "#Classiification with Table 13.1 data\n",
    "print(\"----Classification with Table 13.1 data----\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts_131, df_131['label'], test_size=0.2, shuffle = False ,random_state=69) \n",
    "\n",
    "model = BernoulliNB().fit(X_train, y_train)  \n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "predicted_proba = model.predict_proba(X_test)\n",
    "print(\"Probabilities for each class \")\n",
    "print(predicted_proba)\n",
    "print(\"Accuracy: (0 - 1)\")\n",
    "print(np.mean(predicted == y_test))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion matrix: \")\n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Table 13.10 data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Classification with Table 13.10 data----\n",
      "Probabilities for each class \n",
      "[[0.75 0.25]]\n",
      "Accuracy: (0 - 1)\n",
      "1.0\n",
      "Confusion matrix: \n",
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/28064634/random-state-pseudo-random-numberin-scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import numpy as np\n",
    "\n",
    "#Classiification with Table 13.1 data\n",
    "print(\"----Classification with Table 13.10 data----\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts_1310, df_1310['label'], test_size=0.2, shuffle = False ,random_state=69) \n",
    "\n",
    "model = BernoulliNB().fit(X_train, y_train)  \n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "predicted_proba = model.predict_proba(X_test)\n",
    "print(\"Probabilities for each class \")\n",
    "print(predicted_proba)\n",
    "print(\"Accuracy: (0 - 1)\")\n",
    "print(np.mean(predicted == y_test))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion matrix: \")\n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with IMAL data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Classification with IAML data----\n",
      "Probabilities for each class \n",
      "[[0.61565168 0.38434832]]\n",
      "Accuracy: (0 - 1)\n",
      "1.0\n",
      "Confusion matrix: \n",
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/28064634/random-state-pseudo-random-numberin-scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import numpy as np\n",
    "\n",
    "#Classiification with Table 13.1 data\n",
    "print(\"----Classification with IAML data----\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts_iaml, df_iaml['label'], test_size=0.1, shuffle = False ,random_state=69) \n",
    "\n",
    "model = BernoulliNB().fit(X_train, y_train)  \n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "predicted_proba = model.predict_proba(X_test)\n",
    "print(\"Probabilities for each class \")\n",
    "print(predicted_proba)\n",
    "print(\"Accuracy: (0 - 1)\")\n",
    "print(np.mean(predicted == y_test))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion matrix: \")\n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis ###\n",
    "\n",
    "| Data Table | MultinomialResult | BernoulliResult |\n",
    "| --- | --- | --- |\n",
    "| Table 13.1 | [0.31024139 0.68975861] | [0.80893321 0.19106679]|\n",
    "| Table 13.10 | [0.57142857 0.42857143] | [0.75 0.25]|\n",
    "| IAML data | [0.52228412 0.47771588] | [0.61565168 0.38434832]|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As we can see in previous Table, we obtained similar results for data in Table 13.10 with Bernoulli and Multinomial Classificator.\n",
    "\n",
    "But we get the opposite results using data in Table 13.1 with the differents classificators. This result is consistent with the exercises that we did by hand. \n",
    "\n",
    "Using IAML data, we obtained similar results with both classificators.\n",
    "\n",
    "As we mention before, that probabilities are not the same because of the normalization that the library we are using does.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
